{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is a unit test\n",
    "**Unit** - the smallest workign part that can be tested.\n",
    "* Examples of units: functions, methods, classes, modules, etc.\n",
    "\n",
    "**Unit testing** - software testing method.\n",
    "* Unit testing allows one to scruntinize the correctness of a unit.\n",
    "\n",
    "\n",
    "**Test case** - a set of unit inputs and expected outputs.\n",
    "* Test case summarizes a particular piece of the problem.\n",
    "\n",
    "### Why to use unit tests\n",
    "\n",
    "**Unit tests** - are a foundation for testing \"the bigger picture\" of the software.\n",
    "**Use cases:**\n",
    "* when bugs found\n",
    "* during development\n",
    "* after implemented changes\n",
    "\n",
    "### **How to create a unit test**\n",
    "Step-by-step:\n",
    "1. Decide which units to test\n",
    "2. Define test cases (the creative part):\n",
    "    * \"What are the possible unit outcomes?\"\n",
    "    * \"How can one use the unit?\"\n",
    "    * \"How should the unit behave in all those cases?\"\n",
    "3. Write code for each test case\n",
    "4. Run the tests and analyze the results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating a unit test: example\n",
    "**Unit to test:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for a sum of elements\n",
    "def sum_of_arr(array:list) -> int:\n",
    "    return sum(array)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test cases:\n",
    "1. Input is a list of numbers (as expected) - should return the sum\n",
    "2. Input is an empty list - should return 0\n",
    "3. Input is a list containing one number - should return the number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test Case 1: regular array\n",
    "def test_regular():\n",
    "    assert sum_of_arr([1, 2, 3]) == 6\n",
    "    assert sum_of_arr([100, 150]) == 250\n",
    "\n",
    "# Test Case 2: empty list\n",
    "def test_empty():\n",
    "    assert sum_of_arr([]) == 0\n",
    "\n",
    "# Test Case 3: one number\n",
    "def test_one_number():\n",
    "    assert sum_of_arr([10]) == 10\n",
    "    assert sum_of_arr([0]) == 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytest\n",
    "\n",
    "def factorial(n):\n",
    "    if n == 0: return 1\n",
    "    elif (type(n) == int):\n",
    "        return n * factorial(n-1)\n",
    "    else: return -1\n",
    "\n",
    "# Test case: expected input\n",
    "def test_regular():\n",
    "    assert factorial(5) == 120\n",
    "\n",
    "# Test case: zero input\n",
    "def test_zero():\n",
    "    assert factorial(0) == 1\n",
    "\n",
    "# Test case: input of a wrong type\n",
    "def test_str():\n",
    "    assert factorial('5') == -1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature testing with pytest\n",
    "\n",
    "#### What is a feature test\n",
    "\n",
    "##### Feature\n",
    "* a software system functionality.\n",
    "* satisfies a particular user's requirement.\n",
    "\n",
    "##### Features\n",
    "* are wider that units.\n",
    "* End-user can use features.\n",
    "\n",
    "##### Feature testing\n",
    "* software testing method.\n",
    "* verify the behavior of a specific feature.\n",
    "\n",
    "##### Examples\n",
    "* Data distribution check\n",
    "* Report preparation\n",
    "\n",
    "#### Units vs. features: personal computer\n",
    "**Units:**\n",
    "* Each individual button\n",
    "* Pixels on the screen\n",
    "* LED diods\n",
    "* Blocks on the disk\n",
    "\n",
    "**Features:**\n",
    "* Keyboard\n",
    "* Screen\n",
    "* Illumination\n",
    "* File system\n",
    "\n",
    "### **Why do we use feature tests**\n",
    "#### Feature testing helps:\n",
    "* To test things at the scope of the user interaction with a system.\n",
    "\n",
    "The scope is wider than units:\n",
    "* One unit does not work - does not mean the feature is NOT OK.\n",
    "\n",
    "Vice versa:\n",
    "* All units work as expected - does not mean the feature is OK."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature test example: setup\n",
    "Setup and defining the feature code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup\n",
    "import pandas as pd\n",
    "import pytest\n",
    "df = pd.read_csv('laptops.csv')\n",
    "# Filter feature\n",
    "def filter_data_by_manuf(df, manufacturer_name):\n",
    "    filtered_df = df\\\n",
    "        [df[\"Manufacturer\"] == manufacturer_name]\n",
    "    return filtered_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pytest\n",
    "\n",
    "# Fixture to prepare the data\n",
    "@pytest.fixture\n",
    "def get_df():\n",
    "    return pd.read_csv('https://assets.datacamp.com/production/repositories/6253/datasets/757c6cb769f7effc5f5496050ea4d73e4586c2dd/laptops_train.csv')\n",
    "\n",
    "# Aggregation feature\n",
    "def agg_with_sum(data, group_by_column, aggregate_column):\n",
    "    return data.groupby(group_by_column)[aggregate_column].sum()\n",
    "\n",
    "# Test function\n",
    "def test_agg_feature(get_df):\n",
    "    # Aggregate preparation\n",
    "    aggregated = agg_with_sum(get_df, 'Manufacturer', 'Price')\n",
    "    # Test the type of the aggregated\n",
    "    assert type(aggregated) == pd.Series\n",
    "    # Test the number of rows of the aggregated\n",
    "    assert aggregated.shape[0] > 0\n",
    "    # Test the data type of the aggregated\n",
    "    assert aggregated.dtype in (int, float)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Integration testing with pytest\n",
    "\n",
    "#### What is integration testing?\n",
    "* **Integration testing** - software testing method, that allows to verify that an interaction behaves normally.\n",
    "* **Integration** - an interaction between 2 or more modules inside of a system.\n",
    "\n",
    "**Integrations in real-life projects**\n",
    "Examples:\n",
    "* Power cable\n",
    "* Internet connection\n",
    "* File reading driver\n",
    "* Database connection\n",
    "* Application Programming Interface (API) integration\n",
    "\n",
    "**What can go wrong**\n",
    "\n",
    "Potential integration problems:\n",
    "* Lost connection\n",
    "* Loss of data\n",
    "* Interaction delays\n",
    "* Low bandwidth\n",
    "* Version conflicts\n",
    "* Interface mismatch\n",
    "* Others"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytest, os\n",
    "@pytest.fixture\n",
    "def setup_file():\n",
    "    # Create temporary file\n",
    "    file = \"test_file.txt\"\n",
    "    with open(file, \"w\") as f1:\n",
    "        f1.write(\"Test data 1\")\n",
    "    yield file\n",
    "    os.remove(file)\n",
    "def test_fs(setup_file):\n",
    "    file = setup_file\n",
    "    # Check that the file was created successfully\n",
    "    assert os.path.exists(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary\n",
    "* Integration testing - software testing method; allows to verify that an integration works as expected.\n",
    "* Real-life projects include a lot of different integrations in them.\n",
    "* Integration testing helps to prevent lots of potential problems.\n",
    "* Example: checking integration between Python and a file system."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Performance testing with pytest\n",
    "\n",
    "#### What is performance testing\n",
    "**Performance** - how efficiently does software utilizes the resources of the system to accomplish a task.\n",
    "\n",
    "**Performance Testing** - is a type of testing that measures software performance.\n",
    "\n",
    "#### When performance testing is important\n",
    "##### Resources:\n",
    "* Execution time\n",
    "* CPU\n",
    "* RAM\n",
    "* Other resources\n",
    "\n",
    "##### Cases:\n",
    "* Website speed optimization\n",
    "* App receiving millions of requests\n",
    "* Path planning for a robot vacuum\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Benchmark fixture\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Installation:\n",
    "# !pip install pytest-benchmark\n",
    "# !pip install ipytest\n",
    "\n",
    "\n",
    "# Example_1.py\n",
    "import ipytest\n",
    "import time\n",
    "\n",
    "ipytest.autoconfig()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m.\u001b[0m\u001b[32m                                                                                            [100%]\u001b[0m\n",
      "\n",
      "\u001b[33m------------------------------------------- benchmark: 1 tests ------------------------------------------\u001b[0m\n",
      "Name (time in s)        Min     Max    Mean  StdDev  Median     IQR  Outliers     OPS  Rounds  Iterations\n",
      "\u001b[33m---------------------------------------------------------------------------------------------------------\u001b[0m\n",
      "test_func          \u001b[1m  1.0002\u001b[0m\u001b[1m  1.0005\u001b[0m\u001b[1m  1.0004\u001b[0m\u001b[1m  0.0001\u001b[0m\u001b[1m  1.0005\u001b[0m\u001b[1m  0.0002\u001b[0m       1;0\u001b[1m  0.9996\u001b[0m       5           1\n",
      "\u001b[33m---------------------------------------------------------------------------------------------------------\u001b[0m\n",
      "\n",
      "Legend:\n",
      "  Outliers: 1 Standard Deviation from Mean; 1.5 IQR (InterQuartile Range) from 1st Quartile and 3rd Quartile.\n",
      "  OPS: Operations Per Second, computed as 1 / Mean\n"
     ]
    }
   ],
   "source": [
    "\n",
    "%%ipytest -qq\n",
    "def test_func(benchmark):\n",
    "    benchmark(time.sleep, 1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Benchmark decorator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example_2.py\n",
    "import time\n",
    "def test_func(benchmark):\n",
    "    @benchmark\n",
    "    def sleep_for_1_sec():\n",
    "        time.sleep(1)\n",
    "\n",
    "!pytest Example_2.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Summary\n",
    "* Performance testing - a type of testing that measures the software performance.\n",
    "* Resources are usually finite.\n",
    "* Helpful when resources are constrained.\n",
    "* We can use `pytest-benchmark` fixture by:\n",
    "    * calling `benchmark` directly\n",
    "    * using `@benchmark` as a decorator\n",
    "* The results describe the sample of measured runs in seconds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_list():\n",
    "    return [i for i in range(1000)]\n",
    "def create_set():\n",
    "    return {i for i in range(1000)}\n",
    "def find(it, el=50):\n",
    "    return el in it\n",
    "\n",
    "# Write the performance test for a list\n",
    "def test_list(benchmark):\n",
    "    benchmark(find, it=create_list())\n",
    "\n",
    "# Write the performance test for a set\n",
    "def test_set(benchmark):\n",
    "    benchmark(find, it=create_set())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Speed of Loops\n",
    "\n",
    "Of course, set is better suited for searching elements. It is based on hashes, so you can expect constant complexity most of the time. But what about iterating over all the object's elements? Let's compare the speed of loop iteration over the elements of list and set with pytest and pytest-benchmark. The pytest package has already been imported."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_list(benchmark):\n",
    "\t# Add decorator here\n",
    "    @benchmark\n",
    "    def iterate_list():\n",
    "\t\t# Complete the loop here\n",
    "        for i in [i for i in range(1000)]:\n",
    "            pass\n",
    "\n",
    "def test_set(benchmark):\n",
    "\t# Add decorator here\n",
    "    @benchmark\n",
    "    def iterate_set():\n",
    "        # Complete the loop here\n",
    "        for i in {i for i in range(1000)}:\n",
    "            pass"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is a fixture\n",
    "* Fixture - a prepared environment that can be used for a test execution\n",
    "* Fixture Setup - a process of preparing the environment and setting up resources that are required by one or more tests\n",
    "\n",
    "Imagine preparation for a picnic:\n",
    "1. Invite our friends and prepare the food (that's what fixtures do)\n",
    "2. Have fun!\n",
    "3. Clean up\n",
    "\n",
    "#### Why do we need fixture\n",
    "Fixtures help:\n",
    "* To make test setup easier\n",
    "* To isolate the test of the environmental preparation\n",
    "* To make the fixture code reusable\n",
    "\n",
    "#### Fixture example: overview\n",
    "Assume we have:\n",
    "* a Python `list` variable named `data`\n",
    "* `data = [0, 1, 1, 2, 3, 5, 8, 13, 21]`\n",
    "And we want to test, that:\n",
    "* It contains 9 elements\n",
    "* It contains the elements `5` and `21`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fixture example: code\n",
    "\n",
    "import pytest\n",
    "\n",
    "# Fixture decorator\n",
    "@pytest.fixture\n",
    "# Fixture for data initialization\n",
    "def data():\n",
    "    return [0, 1, 1, 2, 3, 5, 8, 13, 21]\n",
    "\n",
    "def test_list(data):\n",
    "    assert len(data) == 9\n",
    "    assert 5 in data\n",
    "    assert 21 in data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How to use fixtures   \n",
    "To use the fixture we have to do the following:\n",
    "1. Prepare software and tests\n",
    "2. Find \"environment preparation\"\n",
    "3. Create a fixture:\n",
    "    * Declare the `@pytest.fixture` decorator\n",
    "    * Implement the fixture function\n",
    "4. Use the created fixture:\n",
    "    * Pass the fixture name to the test function\n",
    "    * Run the tests!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary\n",
    "We learned about testing fixtures:\n",
    "* Fixture - a prepared environment that can be used for a test execution\n",
    "* We use fixtures to make test setup easier and isolated from the test functions\n",
    "* Simple example: preparation of a Python `list`\n",
    "* Define a pytest fixture by declaring `@pytest.fixture`\n",
    "    * followed by a fixture function\n",
    "* Fixture names are used in the tests as variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preparation\n",
    "In this exercise you will create a fixture and finish test functions. Here, you have a simple data pipeline and some tests to check the data it returns. Since the tests goal is to check that the data is correct, it would be convenient to implement the pipeline as a fixture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m============================= test session starts =============================\u001b[0m\n",
      "platform win32 -- Python 3.11.4, pytest-8.1.1, pluggy-1.4.0\n",
      "rootdir: c:\\projects\\github\\python_tutorials\\Introduction to Testing in Python\n",
      "collected 0 items\n",
      "\n",
      "\u001b[33m============================ \u001b[33mno tests ran\u001b[0m\u001b[33m in 0.00s\u001b[0m\u001b[33m ============================\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Import the pytest library\n",
    "import pytest\n",
    "\n",
    "# Define the fixture decorator\n",
    "@pytest.fixture\n",
    "# Name the fixture function\n",
    "def prepare_data():\n",
    "    return [i for i in range(10)]\n",
    "\n",
    "# Create the tests\n",
    "def test_elements(prepare_data):\n",
    "    assert 9 in prepare_data\n",
    "    assert 10 not in prepare_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of chain requests\n",
    "\n",
    "# Fixture that is requested by the other fixture\n",
    "@pytest.fixture\n",
    "def setup_data():\n",
    "    return \"I am a fixture!\"\n",
    "\n",
    "# Fixture that is requested by the test function\n",
    "@pytest.fixture\n",
    "def process_data(setup_data):\n",
    "    return setup_data.upper()\n",
    "\n",
    "# The test function\n",
    "def test_process_data(process_data):\n",
    "    assert process_data == \"I AM A FIXTURE!\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**How to use chain requests**\n",
    "1. Prepare the program we want to test\n",
    "2. Prepare the testing functions\n",
    "3. Prepare the `pytest` fixtures\n",
    "4. Pass the fixture name to the other fixture signature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fixture requesting other fixture\n",
    "@pytest.fixture\n",
    "def process_data(setup_data):\n",
    "    return setup_data.upper()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary\n",
    "\n",
    "* Chain fixture requests - is a feature that allows a fixture to use another fixture (creating fixture compositions)\n",
    "* It helps to divide the code by functions and keep it modular\n",
    "* Example use case: the steps of data pipeline\n",
    "* To use chain fixture requests pass the fixture name to the other fixture signature"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Chain this out\n",
    "\n",
    "The chain requests can be difficult to understand if you see them for the first time. That's why it is important to feel the order of how the data flows through the pytest functions. Now, you will place them in that order. \n",
    "\n",
    "\n",
    "#### List with a custom length\n",
    "\n",
    "You already saw a list preparation implemented as a fixture. But what if you also want to customize the preparation process? For example, one might want to set a custom length for a generated list. You can implement it with chain fixture requests by making the \"length\" a separate fixture; let's call it list_length(). In the end, you will have a test function that requests the list, and the list is then generated by requesting the list_length()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytest\n",
    "\n",
    "# Define the fixture for returning the length\n",
    "@pytest.fixture\n",
    "def list_length():\n",
    "    return 10\n",
    "\n",
    "# Define the fixture for a list preparation\n",
    "@pytest.fixture\n",
    "def prepare_list(list_length):\n",
    "    return [i for i in range(list_length)]\n",
    "\n",
    "def test_9(prepare_list):\n",
    "    assert 9 in prepare_list\n",
    "    assert 10 not in prepare_list\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chain Fixtures Requests\n",
    "\n",
    "What is a chain request\n",
    "* Chain fixtures requests - a pytest feature, that allows a fixture to use another fixture\n",
    "* Creates a composition of fixtures\n",
    "\n",
    "Why and when to use\n",
    "Chain fixtures requests help to:\n",
    "* **Establish dependencies** between fixtures\n",
    "* Keep the code **modular**\n",
    "  \n",
    "When it can be useful:\n",
    "* When we have several fixtures that **depend on each other**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You're smashing it! While software is getting more and more complicated, you can use such tools as chain fixture requests to make it simpler."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fixtures autouse\n",
    "\n",
    "#### Autouse argument\n",
    "* An optional boolean argument of a fixture\n",
    "* Can be passed to the fixture decorator\n",
    "* When `autouse=True` the fixture function is executing regardless of a request\n",
    "* Helps to reduce the amount of redundant fixture calls\n",
    "\n",
    "#### When to use\n",
    "In case we need to apply certain environment preparations or modifications **for all tests**.    \n",
    "For example, when we want to guarantee, that all tests:   \n",
    "* Have the same data\n",
    "* Have the same connections (data, API, etc.)\n",
    "* Have the same environment configuration\n",
    "* Have a monitor, logging, or profiling    \n",
    "  \n",
    "All such cases should be addressed with an **\"autouse\" argument**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Auto add numbers\n",
    "\n",
    "Now you will practice declaring `autouse`. \n",
    "\n",
    "At the start, you have an empty Python `list`. And there is a function that adds some elements to it. \n",
    "\n",
    "Add `autouse` to the `add_numbers_to_list()` fixture function, so you could use it without explicitly requesting from the test function. \n",
    "\n",
    "And finally complete the assertion tests by checking if `1` is in `init_list` and if `9` is in `init_list`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytest\n",
    "\n",
    "@pytest.fixture\n",
    "def init_list():\n",
    "    return []\n",
    "\n",
    "# Declare the fixture with autouse\n",
    "@pytest.fixture(autouse=True)\n",
    "def add_numbers_to_list(init_list):\n",
    "    init_list.extend([i for i in range(10)])\n",
    "\n",
    "# Complete the tests\n",
    "def test_elements(init_list):\n",
    "    assert 1 in init_list\n",
    "    assert 9 in init_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Another Autouse example\n",
    "\n",
    "# Example of an \"autoused\" fixture:\n",
    "\n",
    "import pytest\n",
    "import pandas as pd\n",
    "\n",
    "# Autoused fixture\n",
    "@pytest.fixture(autouse=True)\n",
    "def set_pd_options():\n",
    "    pd.set_option('display.max_columns', 5000)\n",
    "\n",
    "# Test function\n",
    "def test_pd_options():\n",
    "    assert pd.get_option('display.max_columns') == 5000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary\n",
    "* **Definition of autouse** : An optional boolean argument of a fixture decorator  \n",
    "* **Usage**: `@pytest.fixture(autouse=True)`  \n",
    "* **Advantage**: Helps to reduce the number of redundant fixture calls, thus makes the code simpler  \n",
    "* **Feature**: `autouse=True` the fixture function is executing regardless of a request  \n",
    "* **When to use**: in case we need to apply certain environment preparations or modifications  \n",
    "* **Use cases examples**:\n",
    "    * Reading and preparing data for all tests\n",
    "    * Configuring connections and environment parameters\n",
    "    * Implementing a monitor, a logger, or a profiler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fixtures Teardowns\n",
    "\n",
    "#### What is a fixture teardown\n",
    "* **Fixture Teardown** - a process of cleaning up (\"tearing down\") resources that were allocated or created during the setup of a testing environment.\n",
    "\n",
    "Recall the \"picnic\" analogy:\n",
    "1. Invite our friends and prepare the food\n",
    "2. Have fun!\n",
    "3. **Clean up** - that's the teardown\n",
    "\n",
    "#### Why to use teardowns\n",
    "It is important to **clean the environment** at the end of a test. If one does not use **teardown**, it can lead to significant issues:\n",
    "* Memory leaks\n",
    "* Low speed of execution and performance issues\n",
    "* Invalid test results\n",
    "* Pipeline failures and errors\n",
    "\n",
    "#### **When to use**\n",
    "When to use:\n",
    "* Big objects\n",
    "* More than one test\n",
    "* Usage of autouse\n",
    "  \n",
    "When it is not necessary to use:\n",
    "* One simple script with one test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lazy evaluation in Python\n",
    "# yield - is a Python keyword, which allows to create generators\n",
    "\n",
    "# Example of generator function\n",
    "def lazy_increment(n):\n",
    "    for i in range(n):\n",
    "        yield i\n",
    "f = lazy_increment(5)\n",
    "next(f) # 0\n",
    "next(f) # 1\n",
    "next(f) # 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How to use\n",
    "How to use:\n",
    "* Replace `return` by `yield`\n",
    "* Place the teardown code after `yield`\n",
    "* Make sure that the setup code is only before `yield`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Teardown example\n",
    "@pytest.fixture\n",
    "def init_list():\n",
    "    return []\n",
    "\n",
    "@pytest.fixture(autouse=True)\n",
    "def add_numbers_to_list(init_list):\n",
    "    # Fixture Setup\n",
    "    init_list.extend([i for i in range(10)])\n",
    "    # Fixture output\n",
    "    yield init_list\n",
    "    # Teardown statement\n",
    "    init_list.clear()\n",
    "\n",
    "def test_9(init_list):\n",
    "    assert 9 in init_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary\n",
    "**Definition**: Fixture Teardown - is a process of cleaning up resources that were allocated\n",
    "during the setup.\n",
    "\n",
    "**Usage**:\n",
    "* The yield keyword instead of return\n",
    "* Teardown code after yield\n",
    "\n",
    "**Advantages**:\n",
    "    * Prevents software failures\n",
    "    * Prevents potential drops of performance\n",
    "\n",
    "**When to use**: \n",
    "Always, when you have more than one test!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
